---
title: "HW1"
author: "Haoluan Chen"
date: "9/21/2021"
output:
  pdf_document: default
  html_document: default
---

Name: Haoluan Chen \
Student number: 1003994261 \
Department: Staitstial Science
Program: Master of Statistics\
Year: 1\
Email: haoluan.chen@mail.utoronto.ca\

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Generate pseudorandomm Uniform [0,1]

```{r}
seed <- as.numeric(Sys.time())
m = 2^31 # I chosed m = 2^31 - 1 because it is a very large positive prime number
a = 68626861  # It is a large value and a multiple of 4

b = 1003994261 #It is a odd number, so my LCG is full period/ 
latestval<- floor(print(as.numeric(Sys.time())*1000, digits=15))

```


```{r}
remainder = function(n,m) {
    return( n - m * floor(n/m) )
}

nextrand = function() {
    latestval <<- remainder(a*latestval+b, m)   # (global assignment)
    return(latestval / m)
}


x = rep(0,1000)
for (i in 1:1000)
  x[i] = nextrand()
print("First and Second Moments of a sample of size 1,000:")
print( mean(x) )
print( mean(x^2) )

plot(x)


y = rep(0,1000)
for (i in 1:1000)
  y[i] = nextrand()
print("First and Second Moments of a sample of size 1,000:")
print( mean(y) )
print( mean(y^2) )

plot(y)

cor(x,y)
```

I think my generator seems to be pretty good. First, I ran it twice and found the mean to be close to 0.5 and variance close to 0.33. This is similar to results in Rrng file. Second, two plot shows my points are uniformly distributed between 0 and 1. Third, the correlation between the two set of data that I generated are close to 0. 



# 2) 

```{r}
u1= rep(0,1000)
for (i in 1:1000)
  u1[i] = nextrand()
# generate normal[0,1] 
Z = sqrt(2*log(1/x))* cos(2*pi*u1)
# generate Exp(3)
Y = -log(y)/3

# checking if my normal(0,1) is good
hist(Z)
mean(Z)
mean(Z^2)
# checking if my Exp(3) is good 
mean(Y) # should be 1/3 
mean(Y^2) # should be 1/3^2


```
Normal looks good, but for exp(3) slightly larger variance compared to theoretical variance.

First attempt:

```{r}
mean(abs(Y^2 * Z^5 * sin(Y^3*Z^2)))
se = sd(abs(Y^2 * Z^5 * sin(Y^3*Z^2)))/ 1000
se
```


```{r}
result = rep(0, 20)

q2 <- function(){
x = rep(0,1000)
y = rep(0,1000)
u1= rep(0,1000)
for (i in 1:1000){
  x[i] = nextrand()
  y[i] = nextrand()
  u1[i] = nextrand()
  }
  
Z = sqrt(2*log(1/x))* cos(2*pi*u1) 
Y = -log(y)/3

est <- mean(abs(Y^2 * Z^5 * sin(Y^3*Z^2)))
est
se = sd(abs(Y^2 * Z^5 * sin(Y^3*Z^2)))/ 1000
print(paste("mean =" ,est ,"and se =", se))
}


for (i in 1:20){
  each <- q2()  
  result[i] = as.numeric(substr(each, 8, 20))
}

print(paste("final estiamte:", mean(result)))

```

I don't think my estimate is very accurate because my estimate seems to vary by a lot. For example from some of my run, I saw the lowest estimate is 0.45  and my highest estimate is 0.85, which is almost 2 times the lowest estimate. Secondly, the variance for the exp(3) random variable are not very close to the theoretical variance.  

# 3) 
My student ID is 1003994261, so A = 4 , B = 2, C = 6, D = 1.

```{r}
set.seed(1)
A = 4
B = 2
C = 6
D = 1

g <- function(x1, x2, x3, x4, x5){
  x1^{A+6}*2^{x2+3} * (1 + cos(x1 + 2*x2 + 3*x3 + 4*x4 + (B + 3)*x5))*
    exp((C-12)*x4^2)*exp(-(D+2)*(x4-3*x5)^2)
}

h <- function(x1, x2, x3, x4, x5){
  (x1 + x2^2) / (2 + x3*x4 + x5)
}

q3 <- function(){
M = 10^6

x1list = runif(M)
x2list = runif(M)
x3list = runif(M)
x4list = runif(M)
x5list = runif(M)

numlist = g(x1list, x2list, x3list, x4list, x5list) * 
  h(x1list, x2list, x3list, x4list, x5list)

denomlist = g(x1list, x2list, x3list, x4list, x5list)


se = M^{-1/2}*sd(h(x1list, x2list, x3list, x4list, x5list))
se

print(paste("My estimate is ", mean(numlist)/mean(denomlist), "with se =", se))
}


q3()
q3()
q3()
q3()
q3()
```

I chose all of my x1, x2, x3, x4, x5 to be uniform(0,1) distribution, because it is easy to sample from and it reduces the computational complex of my calculation. 

```{r}
# final esitmate (taking means of one of my runs)
(0.590788435863751 + 0.590569032282988 + 0.590206922048098 
 + 0.591317519358796 + 0.590511405015528)/5
```

I think my program does work well, because my estimates are very consistent (around 0.59). My final estimate is 0.5906787. I think my estimate is pretty accurate because it has consistent and low estimated variance. Also it is close to my estimate in question 4.  



# 4

I want to find a 5-dimensional function that is greater than g(x1, x2, x3, x4, x5) for all xi in (0,1)

my function g is:

$$ x_1^{10}2^{x_2+3}(1+cos[x_1 + 2x_2+3x_3+4x_4+5x_5])e^{-6x_4^2}e^{-3(x_4-3x_5)^2}$$

$(1+cos[x_1 + 2x_2+3x_3+4x_4+5x_5])$ The maximum of this part of the function is equal to 2.

$2^{x_2+3}$ Since $x_2$ is between 0 to 1, so the maximum of this part of the function is 2^4

$e^{-6x_4^2} < 1$  (since $0<x_4, x_5 < 1$)

$e^{-3(x_4-3x_5)^2}< 1$   (since $0<x_4, x_5 < 1$)

Therefore Let $K = 2^4 * 2 $
 
Let $$f(x1) = 1_{0<x_1<1}$$

$$f(x_2) = 1_{0<x_2<1}$$
$$f(x_3) = 1_{0<x_3<1}$$

$$f(x4) = 1_{0<x_4<1}$$
$$f(x5) = 1_{0<x_5<1}$$





```{r}
k = 2^4 * 2
f <- function(x1,x2, x3, x4, x5) {1*x1*x2*x3*x4*x5}
M = 100000


q4<- function(){
hlist = rep(NA, M)
numsamples = 0
x1list= rep(NA, M)
x2list= rep(NA, M)
x3list= rep(NA, M)
x4list= rep(NA, M)
x5list= rep(NA, M)


for (i in 1:M) {
    x1 = runif(1)
    x2 = runif(1)
    x3 = runif(1)
    x4 = runif(1)
    x5 = runif(1)
    X = f(x1,x2, x3, x4, x5)  # sample from f
    U = runif(1)  # for accept/reject
    alpha = g(x1, x2, x3, x4, x5) / (k * 1)  # for accept/reject
    if (U < alpha) {
	    x1list[i] = x1  # keep X value if accepted
		  x2list[i] = x2  # keep X value if accepted
		  x3list[i] = x3  # keep X value if accepted
		  x4list[i] = x4  # keep X value if accepted
		  x5list[i] = x5  # keep X value if accepted
		  hlist[i] = h(x1,x2,x3,x4,x5) # keep h(X) value if accepted
	    numsamples = numsamples + 1
    }
  
}
print(paste(numsamples, mean(hlist, na.rm=TRUE)))
}

for(i in 1:10){
  q4()
}


```

```{r}
#final estimate (taking means of one of my runs)
(0.58712528596649 + 0.585261907096158 + 0.582592897558771 + 0.589129111869184 + 0.590889931925265
 + 0.582426814805924 + 0.583003213887997 + 0.588724870451217 + 0.597622678085176 + 0.59032978185728)/10
```
I think my algorithm does work well, because the number of accepted samples is high (around 300) and my estimate varies only by a bit. Also my estimate is very close to my estimate from q3 around 0.59
